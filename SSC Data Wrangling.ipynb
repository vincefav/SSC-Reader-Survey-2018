{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slate Star Codex Reader Survey 2018\n",
    "\n",
    "This is the data wrangling notebook to accompany my analysis. It's probably not of interest to you unless a) you're a novice data scientist and want to learn some new stuff, or b) you want to see my methodology to understand the strengths and weaknesses of my approach.\n",
    "\n",
    "TL;DR:\n",
    "\n",
    "- I dummify the multiple choice questions. This means I convert them into multiple yes/no (1/0) questions so we can mathematically analyze them.\n",
    "\n",
    "- For questions that allowed an \"Other\" response, I removed responses that appeared fewer than 3 times, and dummified the rest.\n",
    "\n",
    "- I removed responses that used strings of letters when numbers were expected. For instance, writing \"ten\" instead of \"10.\" This happened often enough that's it's not worth the effort to try fixing them all. We just need a stricter survey next time!\n",
    "\n",
    "- I use machine learning to try predicting missing values. This is largely successful.\n",
    "\n",
    "**If you enjoy this, let's connect on LinkedIn:**\n",
    "\n",
    "**https://www.linkedin.com/in/vincefavilla/**\n",
    "\n",
    "I'm a psychologist and machine learning engineer. Also, I'm in the SF Bay Area and looking for a job outside of academia. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('ssc2018public.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open-ended categoricals; people could enter their own text as \"Other\" (bad for data analysis!)\n",
    "# We'll limit them to the most common responses.\n",
    "\n",
    "to_simplify = '''Country\n",
    "State\n",
    "ReligiousDenomination\n",
    "ReligiousBackground\n",
    "PoliticalAffiliation\n",
    "PoliticalChange\n",
    "PoliticalChangeSSC\n",
    "Favoriteblog\n",
    "SSCBenefit\n",
    "SSCChangeMind\n",
    "PoliticalDisagreementI\n",
    "PoliticalDisagreementII'''.split('\\n')\n",
    "\n",
    "# My rule of thumb is \"3 is a pattern.\" So let's remove responses that appear \n",
    "# fewer than 3 times and replace them with \"Other\"\n",
    "for i in to_simplify:\n",
    "    value_counts = data[i].str.lower().value_counts()\n",
    "    value_counts = value_counts[value_counts > 2]\n",
    "    data[i] = np.where(data[i].str.lower().isin(value_counts.index), data[i].str.title(), 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Good categoricals -- need no additional processing\n",
    "to_dummify = '''Race\n",
    "Sex\n",
    "Gender\n",
    "SexualOrientation\n",
    "WorkStatus\n",
    "Profession\n",
    "Degree\n",
    "ReligiousViews\n",
    "ReligiousChange\n",
    "ReligiousChangeII\n",
    "MoralViews\n",
    "LengthofTime\n",
    "AmericanParties\n",
    "PoliticalDisagreementIII\n",
    "TheSystem\n",
    "IncomeStatus\n",
    "IncomeChildhood\n",
    "Cryptocurrency\n",
    "GWWC\n",
    "CharityCareer\n",
    "Depression\n",
    "Anxiety\n",
    "OCD\n",
    "Eatingdisorder\n",
    "Alcoholism\n",
    "Drugaddiction\n",
    "Borderline\n",
    "Bipolar\n",
    "Autism\n",
    "ADHD\n",
    "Schizophrenia\n",
    "Adderall\n",
    "Vegetarian\n",
    "BirthdateEnding\n",
    "Harassment1\n",
    "Harassment2\n",
    "Harassment3\n",
    "Harassment4\n",
    "NaziCategory\n",
    "MarriageCategory\n",
    "Handedness\n",
    "BreatheThrough\n",
    "Placebo\n",
    "PreviousSurveys\n",
    "Mask1\n",
    "Mask2\n",
    "Mask3\n",
    "ParenthesesPalindrome\n",
    "MapRiddle\n",
    "SurgeonRiddle\n",
    "Navon1\n",
    "Navon2\n",
    "Sundown\n",
    "Dancer\n",
    "Dancer2\n",
    "SquaresCircles\n",
    "Tables\n",
    "Cookies\n",
    "Reversal\n",
    "LOC1\n",
    "LOC2\n",
    "LOC3\n",
    "CRTM\n",
    "Wason\n",
    "Referrals\n",
    "PostsRead\n",
    "Comment\n",
    "Subreddit\n",
    "Discord\n",
    "HiddenOpenThreads\n",
    "Meetup\n",
    "PatreonI'''.split('\\n')\n",
    "\n",
    "# Begin dummification!\n",
    "for i in to_dummify:\n",
    "    dums = pd.get_dummies(data[i], prefix=i)\n",
    "    data = pd.concat([data,dums], axis=1)\n",
    "\n",
    "# We've now simplified these, sot they're ready to dummify too.\n",
    "for i in to_simplify:\n",
    "    dums = pd.get_dummies(data[i], prefix=i)\n",
    "    data = pd.concat([data,dums], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I'm going to attempt to turn these questions into numerical scales.\n",
    "# It's not perfect, but I think it's good enough to justify doing.\n",
    "\n",
    "scales = '''Asexuality\n",
    "Relationshipstyle\n",
    "EducationComplete\n",
    "LWID\n",
    "EAID\n",
    "SJID'''.split('\\n')\n",
    "\n",
    "relationship = np.zeros_like(data['Relationshipstyle'])\n",
    "relationship = np.where(data['Relationshipstyle'] == 'Prefer monogamous', 1, relationship)\n",
    "relationship = np.where(data['Relationshipstyle'] == 'Prefer polyamorous', 0, relationship)\n",
    "relationship = np.where(data['Relationshipstyle'] == 'Other', 0.5, relationship)\n",
    "relationship = np.where(data['Relationshipstyle'] == 'Uncertain / no preference', 0.5, relationship)\n",
    "data['Monogamy'] = pd.Series(relationship)\n",
    "\n",
    "data['EducationComplete'] = np.where(data['EducationComplete'] == 'Yes', 1, 0)\n",
    "\n",
    "def yes_no_sorta(col):\n",
    "    new_col = np.zeros_like(data[col])\n",
    "    new_col = np.where(data[col] == 'Yes', 1, new_col)\n",
    "    new_col = np.where(data[col] == 'No', 0, new_col)\n",
    "    new_col = np.where(data[col] == 'Sorta', 0.5, new_col)\n",
    "    return pd.Series(new_col)\n",
    "\n",
    "data['LWID'] = yes_no_sorta('LWID')\n",
    "data['EAID'] = yes_no_sorta('EAID')\n",
    "data['SJID'] = yes_no_sorta('SJID')\n",
    "data['Asexuality'] = yes_no_sorta('Asexuality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Relationship'] = np.where(data['RelationshipStatus'] == 'Single', 0, 1)\n",
    "del data['RelationshipStatus']\n",
    "\n",
    "# I'll make a wild assumption that among people with\n",
    "# 4+ children, the average is 4.5.\n",
    "data['Children'] = data.Children.str.replace(r'4+', '4.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to convert the following columns into numeric values, removing any response that contains non-numeric characters. We're going to lose information the process: imagine someone responding \"about 1400\" vs. \"1400\" for their SAT score; the former is now considered an invalid answer. Due to their sheer amount of data, we have few other options. The survey can be improved next time by forcing numeric responses and instructing people to estimate when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['IQ'] = pd.to_numeric(data['IQ'], errors='coerce')\n",
    "data['Age'] = pd.to_numeric(data['Age'], errors='coerce')\n",
    "data['SATscoresoutof1600'] = pd.to_numeric(data['SATscoresoutof1600'], errors='coerce')\n",
    "data['SATscoresoutof2400'] = pd.to_numeric(data['SATscoresoutof2400'], errors='coerce')\n",
    "data['Income'] = pd.to_numeric(data['Income'], errors='coerce')\n",
    "data['Charity'] = pd.to_numeric(data['Charity'], errors='coerce')\n",
    "data['OlderBrothers'] = pd.to_numeric(data['OlderBrothers'], errors='coerce')\n",
    "data['YoungerBrothers'] = pd.to_numeric(data['YoungerBrothers'], errors='coerce')\n",
    "data['YoungerSisters'] = pd.to_numeric(data['YoungerSisters'], errors='coerce')\n",
    "data['CRT1'] = pd.to_numeric(data['CRT1'], errors='coerce')\n",
    "data['CRT2'] = pd.to_numeric(data['CRT2'], errors='coerce')\n",
    "data['CRT3'] = pd.to_numeric(data['CRT3'], errors='coerce')\n",
    "data['AutismSpectrumTest'] = pd.to_numeric(data['AutismSpectrumTest'], errors='coerce')\n",
    "data['GenderRoleTestM'] = pd.to_numeric(data['GenderRoleTestM'], errors='coerce')\n",
    "data['GenderRoleTestF'] = pd.to_numeric(data['GenderRoleTestF'], errors='coerce')\n",
    "data['Gender2M'] = pd.to_numeric(data['Gender2M'], errors='coerce')\n",
    "data['Gender2F'] = pd.to_numeric(data['Gender2F'], errors='coerce')\n",
    "data['Children'] = pd.to_numeric(data['Children'], errors='coerce')\n",
    "\n",
    "for i in data.columns:\n",
    "    if 'BigFive' in i:\n",
    "        data[i] = pd.to_numeric(data[i], errors='coerce')\n",
    "        data[i] = np.where(data[i] > 100, data[i].mean(), data[i])\n",
    "        data[i] = np.where(data[i] < 0, data[i].mean(), data[i])\n",
    "\n",
    "# Some IQ scores seem a liiittle implausible. Let's replace them with the median.\n",
    "data['IQ'] = np.where(data.IQ > 175, data.IQ.median(), data.IQ)\n",
    "data['IQ'] = np.where(data.IQ < 70, data.IQ.median(), data.IQ)\n",
    "\n",
    "# This question was reversed, so let's fix it\n",
    "data['GlobalWarming'] = 6 - data['GlobalWarming']\n",
    "\n",
    "# Combine these two\n",
    "data['State_Washington Dc'] = data['State_Washington Dc'] + data['State_Washington, Dc']\n",
    "del data['State_Washington, Dc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I'll be ignoring the following features, either because\n",
    "# they're too open-ended, cause errors, or are too specific to SSC\n",
    "\n",
    "to_delete = '''Adderall4\n",
    "BlogReferrals\n",
    "PostReferrals\n",
    "PatreonII\n",
    "PatreonIII\n",
    "PatreonIV\n",
    "Classified\n",
    "PoliticalChangeDescription\n",
    "SurveyTime\n",
    "Adderall2\n",
    "Adderall3\n",
    "Meetup2\n",
    "Favoriteblog\n",
    "Favoritepost'''.split('\\n')\n",
    "\n",
    "for i in to_delete:\n",
    "    del data[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't always like missing data, so we're going to use some machine learning to infer the missing values. I'll make datasets available both with and without this technique, so you can use whichever version you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SATscoresoutof2400</th>\n",
       "      <td>5815.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChildrenHappiness</th>\n",
       "      <td>5472.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IQ</th>\n",
       "      <td>5268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SATscoresoutof1600</th>\n",
       "      <td>4967.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AnxietyScale</th>\n",
       "      <td>4426.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AnxietyFIXED</th>\n",
       "      <td>3256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BigFiveE</th>\n",
       "      <td>2960.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BigFiveC</th>\n",
       "      <td>2953.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BigFiveA</th>\n",
       "      <td>2953.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BigFiveO</th>\n",
       "      <td>2952.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BigFiveN</th>\n",
       "      <td>2951.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender2M</th>\n",
       "      <td>2832.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender2F</th>\n",
       "      <td>2832.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GenderRoleTestM</th>\n",
       "      <td>2657.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AutismSpectrumTest</th>\n",
       "      <td>2654.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GenderRoleTestF</th>\n",
       "      <td>2650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charity</th>\n",
       "      <td>1989.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Income</th>\n",
       "      <td>1749.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentile</th>\n",
       "      <td>1508.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRT1</th>\n",
       "      <td>1081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRT2</th>\n",
       "      <td>991.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRT3</th>\n",
       "      <td>877.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaulRyan</th>\n",
       "      <td>801.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DetailOriented</th>\n",
       "      <td>720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ThoughtProcess</th>\n",
       "      <td>696.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NoisyConversations</th>\n",
       "      <td>693.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phone</th>\n",
       "      <td>693.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NightOwl</th>\n",
       "      <td>692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ClothingTags</th>\n",
       "      <td>689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Risks</th>\n",
       "      <td>687.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HumanBiodiversity</th>\n",
       "      <td>275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Morality</th>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RomanticLife</th>\n",
       "      <td>249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BarackObama</th>\n",
       "      <td>227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rationalistfavorability</th>\n",
       "      <td>215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aesthetics2</th>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FinancialSituation</th>\n",
       "      <td>210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BasicIncome</th>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arrogance</th>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Immigration</th>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MoodScale</th>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SocialSkills</th>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChangeOverOneYear</th>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ambition</th>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoliticalSpectrum</th>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feminism</th>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LifeSatisfaction</th>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIRisk</th>\n",
       "      <td>167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trustworthy</th>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DonaldTrump</th>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Puns</th>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Children</th>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GayMarriage</th>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GlobalWarming</th>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSCAgreement</th>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoliticalInterest</th>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSCFavorability</th>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GenderConformity</th>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0\n",
       "SATscoresoutof2400       5815.0\n",
       "ChildrenHappiness        5472.0\n",
       "IQ                       5268.0\n",
       "SATscoresoutof1600       4967.0\n",
       "AnxietyScale             4426.0\n",
       "AnxietyFIXED             3256.0\n",
       "BigFiveE                 2960.0\n",
       "BigFiveC                 2953.0\n",
       "BigFiveA                 2953.0\n",
       "BigFiveO                 2952.0\n",
       "BigFiveN                 2951.0\n",
       "Gender2M                 2832.0\n",
       "Gender2F                 2832.0\n",
       "GenderRoleTestM          2657.0\n",
       "AutismSpectrumTest       2654.0\n",
       "GenderRoleTestF          2650.0\n",
       "Charity                  1989.0\n",
       "Income                   1749.0\n",
       "Percentile               1508.0\n",
       "CRT1                     1081.0\n",
       "CRT2                      991.0\n",
       "CRT3                      877.0\n",
       "PaulRyan                  801.0\n",
       "DetailOriented            720.0\n",
       "ThoughtProcess            696.0\n",
       "NoisyConversations        693.0\n",
       "Phone                     693.0\n",
       "NightOwl                  692.0\n",
       "ClothingTags              689.0\n",
       "Risks                     687.0\n",
       "...                         ...\n",
       "HumanBiodiversity         275.0\n",
       "Status                    268.0\n",
       "Morality                  267.0\n",
       "RomanticLife              249.0\n",
       "BarackObama               227.0\n",
       "Rationalistfavorability   215.0\n",
       "Aesthetics2               214.0\n",
       "FinancialSituation        210.0\n",
       "BasicIncome               206.0\n",
       "Arrogance                 204.0\n",
       "Immigration               198.0\n",
       "MoodScale                 194.0\n",
       "SocialSkills              183.0\n",
       "ChangeOverOneYear         183.0\n",
       "Ambition                  182.0\n",
       "PoliticalSpectrum         182.0\n",
       "Feminism                  180.0\n",
       "LifeSatisfaction          173.0\n",
       "AIRisk                    167.0\n",
       "Trustworthy               164.0\n",
       "DonaldTrump               161.0\n",
       "Puns                      159.0\n",
       "Children                  148.0\n",
       "GayMarriage               147.0\n",
       "GlobalWarming             133.0\n",
       "SSCAgreement               97.0\n",
       "PoliticalInterest          93.0\n",
       "SSCFavorability            62.0\n",
       "Age                        49.0\n",
       "GenderConformity           40.0\n",
       "\n",
       "[81 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.select_dtypes(include=[np.number])\n",
    "data = data.astype(float)\n",
    "data.columns = data.columns.str.replace('[', '(')\n",
    "data.columns = data.columns.str.replace(']', ')')\n",
    "data.columns = data.columns.str.replace('<', 'lessthan')\n",
    "data.to_csv('ssc2018public_cleaned.csv')\n",
    "\n",
    "# Number of missing values per question\n",
    "missing = pd.DataFrame(data.isnull().sum().sort_values(ascending=False))\n",
    "missing[missing > 0].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regressing Age with an r2 of 0.664\n",
      "regressing GenderConformity with an r2 of 0.391\n",
      "regressing Children with an r2 of 0.449\n",
      "averaging ChildrenHappiness\n",
      "regressing GenderThoughts with an r2 of 0.225\n",
      "averaging IQ\n",
      "averaging SATscoresoutof1600\n",
      "averaging SATscoresoutof2400\n",
      "regressing Percentile with an r2 of 0.201\n",
      "regressing SSCFavorability with an r2 of 0.349\n",
      "regressing SSCAgreement with an r2 of 0.335\n",
      "regressing Rationalistfavorability with an r2 of 0.293\n",
      "regressing PoliticalSpectrum with an r2 of 0.729\n",
      "averaging PoliticalInterest\n",
      "regressing GlobalWarming with an r2 of 0.529\n",
      "regressing Immigration with an r2 of 0.446\n",
      "regressing MinimumWage with an r2 of 0.537\n",
      "regressing GayMarriage with an r2 of 0.519\n",
      "regressing Feminism with an r2 of 0.505\n",
      "regressing HumanBiodiversity with an r2 of 0.303\n",
      "regressing BasicIncome with an r2 of 0.362\n",
      "regressing DonaldTrump with an r2 of 0.574\n",
      "regressing PaulRyan with an r2 of 0.402\n",
      "regressing BarackObama with an r2 of 0.49\n",
      "regressing BernieSanders with an r2 of 0.518\n",
      "averaging Income\n",
      "averaging Charity\n",
      "averaging CharityActivities\n",
      "regressing MoodScale with an r2 of 0.552\n",
      "averaging AnxietyScale\n",
      "regressing LifeSatisfaction with an r2 of 0.669\n",
      "regressing ChangeOverOneYear with an r2 of 0.221\n",
      "regressing ChangeOverTenYears with an r2 of 0.378\n",
      "regressing Ambition with an r2 of 0.368\n",
      "regressing Status with an r2 of 0.413\n",
      "regressing SocialSkills with an r2 of 0.469\n",
      "regressing FinancialSituation with an r2 of 0.431\n",
      "regressing RomanticLife with an r2 of 0.636\n",
      "regressing Morality with an r2 of 0.201\n",
      "averaging Trustworthy\n",
      "averaging Puns\n",
      "regressing AIRisk with an r2 of 0.201\n",
      "averaging OlderBrothers\n",
      "averaging YoungerBrothers\n",
      "averaging OlderSisters\n",
      "averaging YoungerSisters\n",
      "averaging Aesthetics1\n",
      "regressing Aesthetics2 with an r2 of 0.207\n",
      "averaging Arrogance\n",
      "averaging TOA1\n",
      "averaging TOA2\n",
      "averaging TOA3\n",
      "regressing TOU1 with an r2 of 0.305\n",
      "regressing TOU2 with an r2 of 0.292\n",
      "regressing TOU3 with an r2 of 0.276\n",
      "regressing TOU4 with an r2 of 0.488\n",
      "regressing TOU5 with an r2 of 0.307\n",
      "regressing ADH1 with an r2 of 0.301\n",
      "averaging TB\n",
      "averaging CRT1\n",
      "averaging CRT2\n",
      "averaging CRT3\n",
      "regressing Risks with an r2 of 0.274\n",
      "averaging Imagination\n",
      "averaging ThoughtProcess\n",
      "averaging NightOwl\n",
      "averaging DetailOriented\n",
      "averaging NoisyConversations\n",
      "regressing Phone with an r2 of 0.206\n",
      "averaging ClothingTags\n",
      "regressing AutismSpectrumTest with an r2 of 0.516\n",
      "regressing GenderRoleTestM with an r2 of 0.529\n",
      "regressing GenderRoleTestF with an r2 of 0.5\n",
      "averaging Gender2M\n",
      "regressing Gender2F with an r2 of 0.357\n",
      "regressing BigFiveE with an r2 of 0.54\n",
      "regressing BigFiveN with an r2 of 0.526\n",
      "regressing BigFiveA with an r2 of 0.545\n",
      "regressing BigFiveC with an r2 of 0.245\n",
      "regressing BigFiveO with an r2 of 0.318\n",
      "regressing AnxietyFIXED with an r2 of 0.408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Get null columns\n",
    "null_cols = data.isnull().sum()\n",
    "null_cols = list(null_cols[null_cols != 0].index)\n",
    "\n",
    "for i in null_cols:\n",
    "    x = data.fillna(data.median())\n",
    "    y = x.pop(i)\n",
    "    \n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)\n",
    "    reg = XGBRegressor()\n",
    "    reg.fit(xtrain, ytrain)      \n",
    "    pred = reg.predict(xtest)\n",
    "    r2 = r2_score(ytest, pred)\n",
    "    \n",
    "    # If we can reasonably predict these values, do so\n",
    "    if r2 > .20:\n",
    "        print('regressing', i, 'with an r2 of', round(r2, 3))\n",
    "        data['predicted'] = reg.predict(data.fillna(data.median()).drop([i], axis=1))\n",
    "        data[i] = np.where(data[i].isnull(), data['predicted'], data[i])\n",
    "        del data['predicted']\n",
    "    \n",
    "    # Otherwise, just take the median\n",
    "    else:\n",
    "        print('averaging', i)\n",
    "        data[i] = data[i].fillna(data[i].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('ssc2018public_inferred.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
